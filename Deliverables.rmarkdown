---
title: "Deliverables"
author: "Benjamin Lira"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---


Chunk setup `r library(tidyverse)`


```{r}
knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=8, warning = F, message = FALSE,cache = TRUE)

# library(flexdashboard)
require(htmltools)
```


# Deliverable 1

**Data Formats and Transformations I (one-mode)**

**For this deliverable you will need one publication dataset of a topic and time frame of your interest. You can create this file from Scopus as discussed in class.**

**Please explain the dataset used indicating the time frame, the topic and other inclusion criteria like type of publication, for example.**

For this exercise I decided to analyze authorship data on articles regarding self-control. I limited my search to recent articles published in psychology journals.

I will load the data and perform some cleaning.


```{r}
library(tidyverse)
author <- read_csv("scopus.csv")
colnames(author) |> enframe() |> write_csv("colnames.csv")
colnames(author) <- read_csv("colnames_new.csv") |> pull(new)
author <- author |> mutate(authors = str_replace_all(authors, "., Jr.", ". Jr."))

# Create edgelist. One row per relationship.
edge <- 
  author |> 
  # Add rownames for id col
  rownames_to_column(var = "article_id") |> 
  select(article_id,authors, title) |> 
  # Separate authors using comma delimiter
  mutate(authors = str_split(authors, ",")) |> 
  unnest(authors) |> 
  mutate(authors = str_trim(authors))

# Check with auids
author |> 
  select(authors_id) |> 
  mutate(authors_id = str_split(authors_id,";")) |> 
  unnest(authors_id) |> 
  distinct() |> 
  filter(authors_id != "") |> 
  nrow()
# There are 5,888 unique author ids and 5,594 unique author names. To avoid duplicates we'll use the names.


# Make graph
library(igraph)
graph <- 
  graph_from_edgelist(edge |> select(authors, title) |> as.matrix(),
                      directed = FALSE)
V(graph)$type <- V(graph)$name %in% edge$authors

# Get adjacency and incidence
# adjacency <- get.adjacency(graph) |> as.matrix()
incidence <- get.incidence(graph)

# AAT (articles)
aat <- incidence %*% t(incidence)
# ATA (authors)
ata <- t(incidence) %*% (incidence)

# as_tibble(aat)
# as_tibble(ata)
```


1.  **Create a co-authorship network wherein all relationships among coauthors are established. After this, describe the network, how many authors are represented, how many connections are in this network?**

There are `r edge |> count(authors) |> nrow()` unique authors in the data, `r edge |> count(title) |> nrow()` unique articles in the data.

2.  **Who are the top five most prolific authors? What are they publishing about?**

The top five most prolific authors are the following:


```{r}
prolific_authors <- 
 edge |>
 count(authors, sort = T) |> 
 slice_head(n = 5) |> 
 rename(n_articles = n)

prolific_authors |> 
 gt::gt()
```


According to the titles of the articles, reproduced below, they seem to be publishing about psychotherapy, ego depletion, and COVID-19.


```{r}
table <- 
  edge |> 
  filter(authors %in% prolific_authors$authors) |> 
  distinct(title) |> 
  gt::gt()
div(style='height:300px; overflow-y: scroll', table)
```


<br> 3. **Who are the top five most prolific co-authors? What are they publishing about?**

The top five co-author pairs are shown in the table below.


```{r}
# Make a coauthor graph
graph_coauthor <- 
  graph_from_adjacency_matrix(ata,mode = "undirected")

prolific_coauthors <- 
  graph_coauthor |> 
  get.edgelist() |> 
  as_tibble() |> 
  count(V1,V2, sort = T) |> 
  # remove authors with themselves
  filter(V1 != V2) |> 
  rename(n_articles = n) |> 
  slice_head(n = 5)

prolific_coauthors |> 
  gt::gt()
```


According to the titles of the articles, reproduced below, they seem to be publishing about boredom, sports and the dark triad (psychopathy, machiavellism, and narcissim).


```{r}
table <- 
  edge |> 
  filter(authors %in% prolific_coauthors$V1 | authors %in% prolific_coauthors$V2) |> 
  count(title) |> 
  filter(n > 1) |> 
  distinct(title) |> 
  gt::gt()
div(style='height:300px; overflow-y: scroll', table)
```


<br> 4. **How are points 2 and 3 similar or different? In what ways?**

They are actually quite different. This is because the most prolific authors do not necessarily collaborate with the same people all the time, whereas co-authors might be less prolific individually, but more so combined. This really highlights how studying units individually does not reveal the same insights that would be revealed by studying the links between these units.

# Deliverable 2: Data Formats and Transformations I (two-mode)

**For this deliverable you will need the same publication dataset of a topic and time frame of your interest as employed in deliverable 1.**

**Please explain the dataset used indicating the time frame, the topic and other inclusion criteria like type of publication, for example.**

For this exercise, I am using the same dataset as before.

1.  **Create a publication network wherein all relationships link (co-)authors with their respective publication. After this, describe the network, how many authors are represented, how many publications are in this network? And how many connections were established?**

I created that graph in the previous excercise. It contains information from `r edge |> count(authors) |> nrow()` unique authors and, `r edge |> count(title) |> nrow()` unique articles. The connections between authors and their publications amount to `r edge |> nrow()`.


```{r}
graph
```


2.  **Who are the top five most prolific authors? What are they publishing about?**

See above. But reproduced here for convenience. They seem to be publishing about psychotherapy, ego depletion, and COVID-19.


```{r}
prolific_authors |> 
  gt::gt()
```


3.  **What is the publication with the highest number of co-authors? What is this publication about?**


```{r}
most_coauthors <- 
  edge |> 
  group_by(article_id) |> 
  mutate(n_authors = n()) |> 
  ungroup() |> 
  slice_max(n_authors)
```


There is a publication with `r most_coauthors$n_authors[1]` coauthors, the title of which is `r most_coauthors$title[1]`. This makes sense, given that this is a huge paper where multiple labs collaborated to test a multi-site replication of a contentious finding (ego depletion). From what I've heard (I've never read the paper myself), this was a crucial publication in discreting this idea and finding that the finding does not replicate as it should.

4.  **How are points 2 and 3 similar or different? In what ways?**

Very different! Individual level productivity need not align with the amount of co-authors in a given publication.

# Deliverable 3: Data Formats and Transformations I (two-mode to one-mode transformations)

**For this deliverable you will need the same publication dataset of a topic and time frame of your interest as employed in deliverables 1 and 2.**

**In this example, you will start with the two-mode network you created in deliverable 2.**

1.  **Conduct the two-mode to one-mode transformation retaining co-authors.**

See above. Here's the code once more:


```{r}
incidence = graph |> get.incidence()
ata = t(incidence) %*% (incidence)
#ata
```


2.  **What is the meaning of the diagonal in this one-mode transformed matrix? What is the off-diagonal telling us?**

The diagonal reflects the number of articles that author has published. The off diagonal tells us the number of publications the author corresponding to that row shares with the author corresponding to that column. For example, the number at the third row and second column ($x_{32}$ = `r ata[3,2]`) corresponds to the number of shared publications of `r (ata |> as_tibble() |> colnames())[2]` and `r (ata |> as_tibble() |> colnames())[3]`.

**3. Compare the resulting number of authors in this dataset with the total number of authors you obtained in deliverable 1. Are these numbers the same? If they are not, elaborate on why are they different? If they are the same, also elaborate on why they are the same.**

They are the same. The reason is that I am using the same data and exclusion criteria. Also, I did not use the `simplify()` function in the prior exercise. Using it, removes loops (self-nominations, or publications an author shares with him or herself. Otherwise, the diagonals would have been different.

4.  **Tell us how or why these connections represented in this depiction are similar or different from those connections represented in deliverable 1? Would these network structures allow you to address different questions?**

This is the coauthor network which only holds information about number of shared publications among any two co-authors. In deliverable 1, I had a more complete representation of the co-authorship data with a *two-mode* network, which not only included information about co-authorship, but on how this related to article names. As such, any questions regarding the publications themselves and not just the co-authorship information needs the two mode network. For questions pertaining only to shared authorship, the representation here is sufficient.

# Deliverable 4

1.  **Building from the first three deliverables, identify the top six most central human actors in terms of closeness betweenness, eigenvector, and degree centralities. You can present a table with their names.**


```{r}
# Make a coauthor graph
graph_coauthor <- 
  graph_from_adjacency_matrix(ata,mode = "undirected",weighted = TRUE) |> 
  # Remove loops (relationships with self)
  simplify() 

# get centrality
centrality <- tibble(
  author = V(graph_coauthor)$name,
  closeness = closeness(graph_coauthor,normalized = T),
  betweenness = betweenness(graph_coauthor,normalized = T),
  eigenvector = eigen_centrality(graph_coauthor,scale = T)$vector,
  # degree: mode in, out, or total. For undirected graphs, they are the same.
  degree = degree(graph_coauthor,normalized = T, mode = "in")
)

# Top five for each measure
centrality |> 
  slice_min(closeness, n = 6) |> 
  slice(1:6) |> 
  select(author, closeness) |>
  cbind(centrality |> slice_max(betweenness, n = 6) |>slice(1:6) |>  select(author, betweenness)) |>
  cbind(centrality |> slice_max(eigenvector, n = 6) |>slice(1:6) |>  select(author, eigenvector)) |>
  cbind(centrality |> slice_max(degree, n = 6) |>slice(1:6) |>  select(author, degree)) |> 
  repair_names() |> 
  gt::gt() |> 
  gt::fmt_number(c(2,6,8) ,decimals = 3) |> 
  gt::fmt_number(c(4) ,decimals = 3) |> 
gt::tab_footnote("Note. All centrality metrics are normalized to be in the 0 - 1 range.")
```


2.  **Do you see any commonalities among these measures? If so, what does that mean? (go back to lecture notes and/or handbook chapter).**

There are not a lot of commonalities across different centrality measures. This is because different centralities capture differnet aspects. Eigenvector centrality for example, may be high even if degree is low. For example my friend Kaitlyn Werner, has not published many papers with other coauthors (low degree, degree = `r centrality |> filter(author == "Werner K.M.") |> pull(degree)|> round(3)`), but she has co-authored papers with collaborators who are very well connected with the rest of authors (eigenvector centrality = `r centrality |> filter(author == "Werner K.M.") |> pull(eigenvector) |> round(3)`).

3.  **Finally, for degree centrality specify whether you are using in, out, or total and why?**

I specified `mode = "in"` despite the fact that for undirected graphs that argument is ignored. If the graph were a directed graph, then using `mode = "total"` would have overestimated the degree centrality.

Bonus: I want to see centrality distributions. I was working on the other question, and wondered about how these look, because many seem bi-modal.


```{r}
centrality |> 
  pivot_longer(closeness:degree) |> 
  ggplot(aes(value))+
  geom_histogram()+
  facet_grid(~name, scales = "free")+
  egg::theme_article()+
  labs(x = "Value", y = "Count")
```


# Deliverable 5

1.  **Identify the most influential authors replicating Figure 1 (in the presentation) of the Key Actor Analysis (using betweenness and eigenvector centralities).**


```{r}
centrality |> 
  ggplot(aes(betweenness, eigenvector))+
  geom_point(alpha = .4, size = 2)+
  egg::theme_article()+
  labs(x = "Betweenness Centrality", y = "Eigenvector Centrality")+
  geom_smooth(method = "lm")
```


2.  **Replicate Figure 2 (the weighted sociogram in the presentation), making sure that only the top 5% of influential actor names are shown**

Below we add the centrality measures to the network as vertex attributes.


```{r}
V(graph_coauthor)$eigenvector <- centrality$eigenvector
V(graph_coauthor)$degree <- centrality$degree
V(graph_coauthor)$betweenness <- centrality$betweenness
V(graph_coauthor)$closeness <- centrality$closeness
```


Now, we plot the network with these metrics.


```{r}
library(ggnetwork)

isolates <- names(which(degree(graph_coauthor)==0))

GGally::ggnet2(
  graph_coauthor,
  arrow.size = 10,
  arrow.gap = .000025,
  size = V(graph_coauthor)$eigenvector,
  alpha = V(graph_coauthor)$degree,
  #color = ifelse(V(graph_coauthor)$name == "Werner K.M.", "red", "blue"),
) +
  theme(legend.position = "none")
```


3.  **Finally Provide an analysis of Figures 1 and 2 building upon the findings of each figure as shown in class. Specifically, address whether the use of both figures helped you gain a better understanding.**

I find it hard to make deep sense of key actor analysis. In a way it feels like having different watches that provide conflicting times. I think it is helpful to think about this in concrete terms, like I did above for the case of K.M. Werner. Collaborating with many people (degree) need not agree with collaborating with people who are exceptionally well connected (eigenvector), who are the only ones who collaborated with some other author(s) (betweeness), etc.

# Deliverable 6

1.  **Replicate the two-mode interactive network visualization but relying on your publication data. Make sure you add an attribute to the nodes (articles and authors) that go beyond their number. In our examples we added Cumulative GPA, first gen. status for students and course name for courses.** **You can add centrality levels, for example. Make sure you add something meaningful to the line, In our case we added grade in that class. See what you can add :D.**

Let's start by getting the graph and adding some attributes to nodes and links.


```{r}
library(networkD3)
networkD3::simpleNetwork(edge,Source = "authors", Target = "title", zoom = T)

#networkD3::forceNetwork(edge,Source = "authors", Target = "title", zoom = T)
```


# Deliverable 7

**Replicate the one-mode interactive network visualization but relying on your publication data. Here decide whether you want to transform your two-mode to a one-mode elaborating on the implications of each approach (presence or absence of isolates, for example and any implications for visualization)**

**Make sure you add an attribute to the nodes that go beyond their number or ID. In our examples we added Cumulative GPA, first gen. status for students, you can add centrality levels, for example. Make sure you add something meaningful to the line, In our case we number of courses in common. See what you can add as well.**

## Finally, tell us WHY should we even care about doing all of this?

Watching the real connections among units not only

# Deliverable 8

**Please replicate the procedures used in social dependence or peer effects to answer the following questions but using a friendship dataset instead:**


```{r}
# Use this file for the friendship network
idfriend <- "1dwX4kKlx-ctkU0JyH74p3r1w-jJdTAqi"
friendshiplazega <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", idfriend))
dim(friendshiplazega)
matrix = as.matrix(friendshiplazega)

#Reading attributes also provided by Lazega
idattributes <- "1e0GtrRS5PFFNdnd1e4fJcjeuBZ6deF7g"
datattrout <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", idattributes)) |> as_tibble()
```


Let's create some helper functions.


```{r}
# Plot matrix
plot_matrix <- function(matrix){
matrix |> 
  as_tibble() |> 
  rownames_to_column() |> 
  pivot_longer(2:(ncol(matrix)+1)) |> 
  ggplot(aes(rowname, name, fill = value))+
  geom_tile()+
  theme_void()+
  theme(legend.position = "none")}

# Normalize matrix
normalize = function(matrix){return(matrix/rowSums(matrix))}
# Set isolates to zero
isolates_to_zero = function(matrix){matrix[is.na(matrix)] <- 0; return(matrix)}
```


Let's take a look at the normalized and non-normalized matrices.


```{r}
matrix |> plot_matrix()+scale_fill_viridis_c()
matrix |> normalize() |> isolates_to_zero() |> plot_matrix()+scale_fill_viridis_c()
matrix <- matrix |> normalize() |> isolates_to_zero()
```

```{r}
# library(spdep)
# # Analyses
# test.listwAd<-mat2listw(advicelazega)
# moran.test(datattrout$HrRATE90,test.listwAd, zero.policy=TRUE)
# moran.test(datattrout$FeesCollec90,test.listwAd, zero.policy=TRUE)
# #SAR procedures for social dependence as shown in Table 
# fees90Ad <- spatialreg::spautolm(formula = HrRATE90 ~ 1, data = data.frame(datattrout), listw = test.listwAd)
# summary(fees90Ad)
# #Saving the residuals
# hNULL<-residuals(fees90Ad)
# #Testing residuals for sp dependence
# moran.test(hNULL,test.listwAd, zero.policy=TRUE)
# fees90Ad <- spatialreg::spautolm(formula = FeesCollec90 ~ 1, data = data.frame(datattrout), listw = test.listwAd)
# summary(fees90Ad)
# #Saving the residuals
# hNULL<-residuals(fees90Ad)
# #Testing residuals for sp dependence
# moran.test(hNULL,test.listwAd, zero.policy=TRUE)
# fees90Ad <- spatialreg::spautolm(formula = HrRATE90 ~ partner , data = data.frame(datattrout), listw = test.listwAd)
# summary(fees90Ad)
# hNULL<-residuals(fees90Ad)
# #Testing residuals for sp dependence
# moran.test(hNULL,test.listwAd, zero.policy=TRUE)
# #Getting socially lagged indicators as explained in the chapter
# datattrout$lag.partnersAd <- lag.listw(test.listwAd, datattrout$partner, zero.policy=T, na.action=na.omit)
# #Network visualization
# ###Visualizing Advice network. This reproduces Figure 5(a)
# g1c<-graph.adjacency(advicelazegac)
# cent2<-data.frame(totaldegree=degree(g1c, mode="total"), indegree=degree(g1c, mode="in") , outegree=degree(g1c, mode="out"), bet=round(betweenness(g1c, directed=F, normalized = TRUE, weights=NA),3),eig=round(evcent(g1c, weights=NA)$vector,3), closeness=round((1/closeness(g1c))/max(1/closeness(g1c)), 3))
# 
# set.seed(47)
# l2<-layout_with_graphopt(g1c, niter=5000, charge = 0.1)
# cent2
# V(g1c)$size<-abs((cent2$eig)/max(cent2$eig))*15#The divisor is the highest EC
# library(classInt)
# library(RColorBrewer)
# nclr<-5
# plotvar2 <- cent2$bet
# plotclr2 <- brewer.pal(nclr, "Greys")
# class2 <- classIntervals(plotvar2, nclr, style = "quantile")
# colcode2 <- findColours(class2, plotclr2, digits = 3)
# library(igraph)
# x11()
# par(bg="grey20")
# plot(g1c, vertex.color=colcode2, vertex.label=datattrout$id, layout=l2, vertex.label.color=rev(colcode2), edge.arrow.size=.25, edge.color=rgb(255, 255, 0, max=
# 255, 255/2))
# legend("bottomright", legend = names(attr(colcode2, "table")), fill = attr(colcode2, "palette"), title="Bet Centrality", bg=rgb(255, 255, 0, max=255, 255/2))
# title(main="Sociogram Advice Network (Lazega, 2001)\nSize conditional on EV, color indicates bet centrality", col.main="grey77", col.sub="grey77", cex.sub=1,cex.main=1.5, font.sub=2)

```


1.  **Is there evidence of stronger dependence in the per hour rate compared to the fees brought in 1990 amounts?**


```{r}
library(spdep)
# Create listweights object
test.listwAd<-mat2listw(matrix)

# Moran's I for outcome dependence
moran.test(datattrout$HrRATE90,test.listwAd, zero.policy=TRUE)
moran.test(datattrout$FeesCollec90,test.listwAd, zero.policy=TRUE)
```


Moran's I for per hour rate (`HrRATE90`) is .52, which is higher than that for fees brought in 1990 (`FeesCollec90`, Moran's I = .46). This means that if a lawyer's friends are charging more per hour, nad bringing in more money on fees, that particular lawyer is likely to have higher than average hour rate and fees in 1990. This effect seems to be stronger for hour rate than for fees though.

See plots below.


```{r}
moran.plot(datattrout$HrRATE90,test.listwAd, zero.policy=TRUE, xlab = "Hour Rate", ylab = "Lagged Hour Rate")
moran.plot(datattrout$FeesCollec90,test.listwAd, zero.policy=TRUE, xlab = "Fees", ylab = "Lagged Fees")
```


2.  **Did you have to modify the empty model to correct for spatial dependence? That is, was the dependence issue addressed with the empty regression models? Explain the likely mechanisms behind this process.**

Let's run the empty model, and look at whether that fixes the spatial dependence issue. We run the spatial regression, then extract the residuals, and test whether moran's I is no longer significant.

### Hour Rate


```{r}
# Unadjusted mean
datattrout$HrRATE90 |> mean()

#SAR procedures for social dependence as shown in Table 
hour_rate <- spatialreg::spautolm(formula = HrRATE90 ~ 1, data = data.frame(datattrout), listw = test.listwAd)
summary(hour_rate)
#Saving the residuals
residuals <- residuals(hour_rate)
#Testing residuals for sp dependence
moran.test(residuals,test.listwAd, zero.policy=TRUE)
```


We see that accounting for spatial dependence, the mean is 127.9, as opposed to the raw, unadjusted mean of 154.507. The residuals from the adjusted mean controlling for dependece still show a significant moran's test, sugggesting that the dependence issue is not yet solved (Moran's I = -.01, p = .430)

### Fees Collected


```{r}
# Unadjusted mean
datattrout$FeesCollec90 |> mean()

#SAR procedures for social dependence as shown in Table 
fees <- spatialreg::spautolm(formula = FeesCollec90 ~ 1, data = data.frame(datattrout), listw = test.listwAd)
summary(fees)
#Saving the residuals
residuals <- residuals(fees)
#Testing residuals for sp dependence
moran.test(residuals,test.listwAd, zero.policy=TRUE)
```


We see that accounting for spatial dependence, the mean is 141650, as opposed to the raw, unadjusted mean of 176813.3. The residuals from the adjusted mean controlling for dependence no longer show a significant Moran's test, suggesting that the dependence issue is solved for this outcome (Moran's I = -.01, p = .446).

3.  **Are there any lagged indicators that you may be interested in testing?**

Lets look at lag partner:


```{r}
datattrout <- 
  datattrout |> 
  mutate(lag_partner = lag.listw(test.listwAd,partner, zero.policy = T, na.action = na.omit))

# Hour Rate
hour_rate <- spatialreg::spautolm(formula = HrRATE90 ~ lag_partner, data = data.frame(datattrout), listw = test.listwAd)
summary(hour_rate)
#Saving the residuals
residuals <- residuals(hour_rate)
#Testing residuals for sp dependence
moran.test(residuals,test.listwAd, zero.policy=TRUE)

# Fees
fees <- spatialreg::spautolm(formula = FeesCollec90 ~ lag_partner, data = data.frame(datattrout), listw = test.listwAd)
summary(fees)
#Saving the residuals
residuals <- residuals(fees)
#Testing residuals for sp dependence
moran.test(residuals,test.listwAd, zero.policy=TRUE)

```


We can see that in both cases, having friends who are partners predicts making more money (B = 139,329, p \< .001) and charging more per hour (B = 94, p \< .001). In fees, adding lag partner makes lambda not significant anymore, but the opposite is the case for hourly rate.

4.  **After removing isolates: How many higher order neighbors did you find? Does this change by outcome?**

To remove isolates we have to find rows and columns that are all zero (no relationships).


```{r}
isolates = which((matrix |> rowSums()) == 0)
connected = setdiff(1:71, isolates)

no_isolates <- matrix[connected,connected]
test.listwAd<-mat2listw(no_isolates)

datattrout <- datattrout |> slice(connected)
```


As shown in class, should be same result:


```{r}
sublist <- subset(test.listwAd[[2]], subset = card(test.listwAd[[2]])>0)
# It matches
```


Let us test to see if wee need higher order neighbors using the `spcor` function


```{r}
neighbors <- test.listwAd[[2]]
sp.correlogram(neighbors,datattrout$HrRATE90, order = 6, method = 'I', zero.policy = T) |> plot(main = "Moran's I for Different Number of Lags for Hour Rate")
sp.correlogram(neighbors,datattrout$FeesCollec90, order = 6, method = 'I', zero.policy = T) |> plot(main = "Moran's I for Different Number of Lags for Fees")
```


In both cases, there is outcome dependence until the second lag, meaning, both direct friends and friends of friends have dependent fees and rates. Friends of friends of friends (and so on) are no longer influential.

5.  **How do the Moran's I estimates change when removing isolates**

Let's calculate Moran's I without isolates.


```{r}
# Rate
moran.test(datattrout$HrRATE90,test.listwAd, zero.policy=TRUE)
moran.plot(datattrout$HrRATE90,test.listwAd, zero.policy=TRUE, xlab = "Hour Rate", ylab = "Lagged Hour Rate")

# Fees
moran.test(datattrout$FeesCollec90,test.listwAd, zero.policy=TRUE)
moran.plot(datattrout$FeesCollec90,test.listwAd, zero.policy=TRUE, xlab = "Fees", ylab = "Lagged Fees")
```


6.  **Are there changes by outcome?**

Removing isolates results in larger Moran's I indicators for each outcome.

See table below for change in Moran's I with and without isolates.

| Outcome | With Isolates | Without Isolates | Change   |
|---------|---------------|------------------|----------|
| Rate    | .52           | .67              | .15, 29% |
| Fees    | .46           | .52              | .06, 13% |

The change is more important for rates as opposed to fees.

# Deliverable 9

1.  **Test for outcome dependence using author's number of publication as a function of her/his coauthors' number of publications**

First lets create the objects we need


```{r}
matrix <- get.adjacency(graph_coauthor) |> as.matrix()
matrix <- matrix |> normalize() |> isolates_to_zero()
weights <- mat2listw(matrix)

counts = diag(ata) |> enframe()

```


...and run Moran's I.


```{r}
moran.test(counts$value,weights, zero.policy=TRUE)

```


There is evidence for outcome dependence, Moran's I = .29, p \< .001.

2.  **Also visualize this influence level, what may be the mechanism to explain these clusters and outliers?**


```{r}
moran.plot(counts$value,weights, zero.policy=TRUE, xlab = "Number of Publications", ylab = "Lagged Number of Publications")
```


There aren't any clear outlying data. There are a few outliers visible in the bottom of the plot, and a couple of authors clustered at a single publication, with co-authors that have authored any number of publications. This is likely caused by that publication mentioned earlier that has more than 100 authors.

3.  **Test how many higher order neighbors should we account for in this framework?**


```{r}
neighbors <- weights[[2]]
number_lags <- sp.correlogram(neighbors,counts$value, order = 6, method = 'I', zero.policy = T)
number_lags
number_lags |> plot(main = "Moran's I for Different Number of Lags for Number of Publications")
```


The model suggests that all the way up to 6 lags remains significant. However, there are diminishing returns, with one lag (Moran's I = .29), and two lags (Moran's I = .15), being higher than 3 or more lags (Moran's I between .09 - .05)

4.  **If needed create a higher order weight matrix and test for autocorrelation**

Let's keep lags all the way to 3 lagged coauthors and test for auto correlation. (This should produce the same result as `sp.correlogram`).


```{r}
nth_order <- nblag(neighbors, maxlag=3)
nth_order <- nblag_cumul(nth_order)
nth_order <- nb2listw(nth_order, style="W", zero.policy=T)
moran.test(counts$value, nth_order, zero.policy=TRUE)
```


5.  **Finally, test whether an empty model may be enough to address social dependence**


```{r}
# Unadjusted mean
counts$value |> mean()

#SAR procedures for social dependence as shown in Table 
num_pubs <- spatialreg::spautolm(formula = value ~ 1, data = counts, listw = weights)
summary(num_pubs)
#Saving the residuals
residuals <- residuals(num_pubs)
#Testing residuals for sp dependence
moran.test(residuals,weights, zero.policy=TRUE)
```


The empty model correctly addresses social dependence. The original unadjusted mean is of 1.22 publications per author. Using SAR, the mean gets adjusted down to 1.17, with $\lambda$ = 0.16, meaning that for every additional publication of your co-authors, you are expected to have .16 more publications. The residuals for this model reveal no evidence for social dependence (Moran's I = - .001, p = .543), suggesting that our residuals are now iid.

# Deliverable 10: Geographical and spatial network analyses

**The example shown in class measured the spatial dependence of states' Economic Performance Rankings as of 2011. This ranking is a backward-looking measure based on a state's performance on three important variables: Personal Income Per Capita, Absolute Domestic Migration, and Non-Farm Payroll Employment---all of which are highly influenced by state policy. This ranking details states' individual performances over the past 10 years based on this economic data (see https://www.alec.org/app/uploads/2011/11/RSPS_4th_Edition.pdf Links to an external site., Table 8 on page 49).**

**For the tenth assignment you should update the results of maps 1 and 2 with the most recent information found in the Rich States Poor States 15th Edition https://www.richstatespoorstates.org/app/uploads/2022/04/2022-15th-RSPS.pdf Links to an external site.**

**Please use the ranks provided in the table called "ALEC-Laffer State Economic Performance Rankings, 2010-2020" on page 4 to address the following questions.**

**The questions to be addressed are:**

1.  **Does the spatial dependence based on neighboring structure remain significant at the .05 significance level with the newest data?**

    sdf

2.  **Did the spatial dependence based on randomly generated ties yield similar inferences?**

sdf

# Deliverable 11

**For the Eleventh assignment choose in-state tuition (Tuition2) as the outcome variable and test whether the Moran's I using the 1k neighbor's structure yielded significant results.**

**Repeat the approach using a 75-mile radius approach and also test for dependence of this in-state indicator.**

**Questions**:

1.  **What Moran's I indicator (1k or 75-miles) is stronger?**

sdf

2.  **Why would this be the case?**

sdf

3.  **Please plot the two corresponding maps**

sdf

# Final

Final paper guidelines (Between 750 and 1,500 words)

1\. Study objectives or purpose

• Introduction, description of the problem, motivation

2\. Study methods or modes of inquiry

3\. Proposed Data sources or evidence, data collection, participants

4\. Study significance

• Importance of the topic

• The originality of the work

• The quality of theoretical or conceptual frameworks and data sources

• Soundness of the research design, analysis, and/or interpretation

• Evidence that the paper can be completed

Note: The course relies heavily on code. This may be tedious and/or frustrating for

some students, we have to learn to deal with frustration as we go through the exercises.

This is a normal process, specially for R beginners.

We are a learning community and as such we all should respect our opinions. Our

presence and participation are significant to the life of the class, to our learning, and

to your grade. In the case of borderline grades, your engagement in and contribution

to the class will be factored in.

